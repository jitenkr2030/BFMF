# Strategic Partnership Program for India's AI Sovereignty

## Empowering India's AI Future Through Collaboration

The Bharat Foundation Model Framework (BFMF) represents a landmark initiative in India's journey towards AI independence. As a strategic, community-driven open-source project, we are developing sovereign foundation models that are uniquely Indian - trained on our data, built with cutting-edge open technologies, and deployed on domestic infrastructure.

To accelerate this national mission, we invite leading institutions, technology companies, research organizations, and visionary individuals to join us as strategic partners in building India's AI future.

## Partnership Opportunities

| Category | Strategic Value | Partner Examples |
|----------|----------------|------------------|
| **Computing Infrastructure** | High-performance GPU/TPU clusters for model training and optimization | ExoStack, Cloud Providers, HPC Centers |
| **Data Partnerships** | Curated datasets spanning Indian languages, domains, and use-cases | Research Institutions, Government Agencies |
| **Research & Development** | Advanced model architectures and domain-specific innovations | AI Institutes, Universities, R&D Labs |
| **Strategic Investment** | Sustainable funding for compute, talent, and infrastructure | Corporations, Investment Firms, Grants |
| **Technical Ecosystem** | Engineering expertise, tools, and platform capabilities | Technology Companies, Developer Communities |

## Partnership Benefits

### Strategic Advantages
- **Priority Access**: Early preview and integration rights for new Bharat models
- **Co-Innovation**: Joint R&D opportunities and technology collaboration
- **Market Leadership**: First-mover advantage in India's emerging AI ecosystem

### Institutional Benefits
- **Brand Recognition**: Featured placement on BFMF's partner ecosystem
- **Technical Support**: Dedicated integration and optimization assistance
- **Research Publications**: Co-authored papers and technical documentation

## Current Strategic Requirements (Q4 2025)

| Resource | Scope | Strategic Impact |
|----------|--------|------------------|
| Computing Infrastructure | 16-32 NVIDIA A100/H100 GPUs | Enable Bharat-Base (7B) model training |
| Storage Infrastructure | 50TB+ NVMe/Cloud Storage | Support dataset and model hosting |
| Financial Resources | â‚¹10-20L Investment | Scale compute and research operations |
| Research Partnerships | 5-10 Technical Partners | Drive model optimization and evaluation |

## Become a Strategic Partner

Join us in shaping India's AI future. Connect with our partnership team:

- **Email**: partnerships@bharat-ai.org
- **Strategic Discussions**: [Partner Inquiry Portal](https://github.com/bharat-ai/partnerships)
- **Partnership Portal**: https://bharat-ai.org/partners (launching soon)

Our team will provide detailed information about partnership opportunities, technical requirements, and engagement frameworks.

---

> "Every partnership strengthens India's path to AI sovereignty. Together, we're not just adopting AI - we're defining its future with Indian innovation and values."

Join us in building India's sovereign AI capabilities. The future of AI is being written in Bharat.

# ğŸ‡®ğŸ‡³ Bharat Foundation Model Framework (BFMF)

<div align="center">

**India's Open-Source Ecosystem for Building, Training, and Deploying Foundation Models**

[![License](https://img.shields.io/badge/License-Apache%202.0%20%2F%20BOAL-blue.svg)](LICENSE)
[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red.svg)](https://pytorch.org)
[![Docs](https://img.shields.io/badge/Docs-Coming%20Soon-green.svg)](docs/)
[![Community](https://img.shields.io/badge/Community-Join%20Us-orange.svg)](https://github.com/bharat-ai/bharat-fm/discussions)

</div>

---

## ğŸ§­ Vision

> To empower India's AI independence by providing an open, modular, and scalable foundation model framework â€” enabling developers, researchers, and institutions to build **Indian-language, domain-specific, and locally governed AI systems**.

---

## âš™ï¸ Core Philosophy

* ğŸ‡®ğŸ‡³ **Built for Bharat** â€“ Native support for Indian languages, datasets, and regional diversity.
* ğŸ§  **Foundation First** â€“ Focused on base model pretraining, fine-tuning, and adaptation.
* ğŸ”’ **Sovereign AI** â€“ 100% self-hostable, privacy-first, and data-residency compliant.
* ğŸŒ **Open Collaboration** â€“ Interoperable with Hugging Face, OpenAI-style APIs, and ExoStack.
* ğŸ§© **Modular Stack** â€“ Each layer can work independently or as part of a complete pipeline.

---

## ğŸ—ï¸ High-Level Architecture

```
 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
 â”‚                BharatFM Stack                 â”‚
 â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
 â”‚ 7. Governance & Registry (MLflow, Audit, ACL) â”‚
 â”‚ 6. Serving & Deployment (vLLM, ExoStack)      â”‚
 â”‚ 5. Evaluation & Benchmark (HELM, lm-eval)     â”‚
 â”‚ 4. Fine-tuning Interface (Axolotl, LoRA)      â”‚
 â”‚ 3. Training Engine (Deepspeed, Megatron)      â”‚
 â”‚ 2. Model Architectures (GLM, LLaMA, Mistral)  â”‚
 â”‚ 1. Data Layer (Indic Corpora, RedPajama)      â”‚
 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

![BFMF Architecture](docs/architecture.png)

---

## ğŸ§© Modular Components

| Module | Description | Tools / Dependencies |
| --- | --- | --- |
| **bharat_data** | Prepares multilingual datasets, tokenizers, and cleaning pipelines | Hugging Face Datasets, IndicNLP, Dolma |
| **bharat_model** | Defines base model architectures (decoder-only, encoder-decoder, mixture-of-experts) | PyTorch, Transformers, Megatron-LM |
| **bharat_train** | Distributed pretraining and fine-tuning pipeline | Deepspeed, Axolotl, FSDP |
| **bharat_eval** | Evaluation and benchmarking suite | HELM, lm-eval-harness, OpenCompass |
| **bharat_deploy** | Serving layer using vLLM or ExoStack | FastAPI, Triton, vLLM |
| **bharat_registry** | Model registry, versioning, and experiment tracking | MLflow, Hugging Face Hub |
| **bharat_cli** | CLI toolkit for job scheduling, config management | Typer, ExoCLI |

---

## ğŸ§  Supported Model Families

| Model | Type | Size | Purpose |
| --- | --- | --- | --- |
| **Bharat-Base** | Decoder-only | 1.3B / 7B | General-purpose pre-trained model |
| **Bharat-Lite** | 1.3B | On-device / low-resource | |
| **Bharat-MoE** | Mixture of Experts | 12Ã—7B | Scalable modular architecture |
| **Bharat-Gov** | Finetuned | Governance, policy, public data | |
| **Bharat-Edu** | Finetuned | Education, tutoring, content generation | |
| **Bharat-Lang** | Finetuned | Multilingual & translation tasks | |

---

## ğŸŒ Integration with Other Frameworks

| Layer | Integrated With |
| --- | --- |
| Compute | ExoStack |
| Training | Axolotl, Deepspeed |
| Inference | vLLM, ExoServe |
| Registry | MLflow, Hugging Face Hub |
| Dataset | IndicNLP, RedPajama, Dolma |

---

## ğŸš€ Quick Start

### Prerequisites

- Python 3.8+
- PyTorch 2.0+
- CUDA 11.8+ (for GPU training)
- 16GB+ RAM (for 1.3B model training)

### Installation

```bash
# Clone the repository
git clone https://github.com/bharat-ai/bharat-fm.git
cd bharat-fm

# Install in development mode
pip install -e ".[dev]"

# Or install with specific extras
pip install -e ".[train,eval,deploy]"
```

### Basic Usage

#### 1. Train Bharat-Lite (1.3B) for Hindi-English Chat

```bash
bharat train --model glm --dataset indic_mix --steps 50000
```

#### 2. Fine-tune Bharat-Gov for Policy AI

```bash
bharat finetune --model bharat-base --lora --dataset govt_data
```

#### 3. Deploy via ExoStack

```bash
bharat deploy --model bharat-gov --infra exostack --replicas 3
```

#### 4. Evaluate Model Performance

```bash
bharat eval --model bharat-base --benchmark helm --languages hi,en
```

---

## ğŸ“ Project Structure

```
bharat-fm/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSE
â”œâ”€â”€ pyproject.toml
â”œâ”€â”€ requirements.txt
â”‚
â”œâ”€â”€ bharat_data/
â”‚   â”œâ”€â”€ datasets/
â”‚   â”œâ”€â”€ tokenizers/
â”‚   â””â”€â”€ preprocess.py
â”‚
â”œâ”€â”€ bharat_model/
â”‚   â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ modeling_glm.py
â”‚   â”œâ”€â”€ modeling_llama.py
â”‚   â””â”€â”€ modeling_moe.py
â”‚
â”œâ”€â”€ bharat_train/
â”‚   â”œâ”€â”€ trainer.py
â”‚   â”œâ”€â”€ finetune.py
â”‚   â””â”€â”€ deepspeed_config.json
â”‚
â”œâ”€â”€ bharat_eval/
â”‚   â”œâ”€â”€ benchmarks/
â”‚   â””â”€â”€ evaluator.py
â”‚
â”œâ”€â”€ bharat_deploy/
â”‚   â”œâ”€â”€ api.py
â”‚   â””â”€â”€ inference_server.py
â”‚
â”œâ”€â”€ bharat_registry/
â”‚   â””â”€â”€ mlflow_utils.py
â”‚
â”œâ”€â”€ bharat_cli/
â”‚   â””â”€â”€ main.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ architecture.png
â”‚   â””â”€â”€ user_guide.md
â”‚
â”œâ”€â”€ examples/
â”‚   â”œâ”€â”€ hello_bharat.py
â”‚   â””â”€â”€ config_examples/
â”‚
â””â”€â”€ tests/
    â”œâ”€â”€ test_data.py
    â”œâ”€â”€ test_model.py
    â””â”€â”€ test_train.py
```

---

## ğŸ“˜ Example Use Cases

### 1. Multilingual Chatbot

```python
from bharat_model import BharatLite
from bharat_deploy import InferenceServer

# Load pre-trained model
model = BharatLite.from_pretrained("bharat-ai/bharat-lite-1.3b")

# Create inference server
server = InferenceServer(model, host="0.0.0.0", port=8000)
server.start()
```

### 2. Custom Fine-tuning

```python
from bharat_train import FineTuner
from bharat_data import IndicDataset

# Load dataset
dataset = IndicDataset("hindi_english_pairs")

# Initialize fine-tuner
finetuner = FineTuner(
    base_model="bharat-base",
    lora_rank=16,
    learning_rate=2e-5
)

# Fine-tune
finetuner.train(dataset, epochs=3)
```

### 3. Model Evaluation

```python
from bharat_eval import Evaluator
from bharat_model import BharatBase

# Load model
model = BharatBase.from_pretrained("bharat-base")

# Initialize evaluator
evaluator = Evaluator(model)

# Run benchmarks
results = evaluator.evaluate(
    benchmarks=["helm", "lm-eval"],
    languages=["hi", "en", "bn"]
)

print(results)
```

---

## ğŸ› ï¸ Development

### Setting up Development Environment

```bash
# Clone and install
git clone https://github.com/bharat-ai/bharat-fm.git
cd bharat-fm
pip install -e ".[dev]"

# Install pre-commit hooks
pre-commit install

# Run tests
pytest tests/

# Run linting
ruff check bharat_/
black bharat_/
```

### Contributing

We welcome contributions! Please see [CONTRIBUTING.md](CONTRIBUTING.md) for guidelines.

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

---

## ğŸš€ Roadmap (Phase-wise)

| Phase | Goal | Timeline |
| --- | --- | --- |
| Phase 1 | Base repo setup + data pipeline + model configs | âœ… Nov 2025 |
| Phase 2 | Training engine + Axolotl integration | Dec 2025 |
| Phase 3 | Inference + deployment (ExoStack integration) | Jan 2026 |
| Phase 4 | Launch Bharat-Lite (1.3B multilingual) | Mar 2026 |
| Phase 5 | Community datasets + fine-tuned variants | Mid 2026 |

---

## ğŸ›ï¸ Governance

### Steering Committee
- **Technical Lead**: AI Research Institute, India
- **Community Lead**: Open Source India Foundation
- **Industry Lead**: Bharat AI Consortium

### Decision Making Process
- [RFC Process](docs/rfcs/)
- [Community Meetings](community/meetings/)
- [Technical Advisory Board](governance/advisory-board.md)

---

## ğŸ“„ License

This project is licensed under the **Apache 2.0 / Bharat Open AI License (BOAL)**.

- âœ… **Commercial usage allowed**
- âœ… **Academic usage allowed** 
- âœ… **Modification and distribution allowed**
- ğŸ“ **India-first attribution required**

See [LICENSE](LICENSE) for the full license text.

---

## ğŸ¤ Community & Support

- **GitHub Discussions**: [Join the conversation](https://github.com/bharat-ai/bharat-fm/discussions)
- **Discord Community**: [Join our Discord](https://discord.gg/bharat-ai)
- **Documentation**: [Read the docs](docs/)
- **Issues**: [Report bugs or request features](https://github.com/bharat-ai/bharat-fm/issues)

---

## ğŸ™ Acknowledgments

- **Government of India** - For supporting sovereign AI initiatives
- **Indian AI Research Community** - For technical guidance and expertise
- **Open Source Community** - For building the foundation we build upon
- **Hugging Face** - For the amazing transformers ecosystem
- **ExoStack** - For compute infrastructure integration

---

## ğŸ“ˆ Citation

If you use BFMF in your research, please cite:

```bibtex
@software{bharat_fmf_2025,
  title={Bharat Foundation Model Framework: India's Open-Source Ecosystem for Sovereign AI},
  author={Bharat AI Team},
  year={2025},
  url={https://github.com/bharat-ai/bharat-fm},
  license={Apache 2.0 / BOAL}
}
```

---

<div align="center">

**ğŸ‡®ğŸ‡³ Made with â¤ï¸ for Bharat's AI Independence**

[![Back to top](https://img.shields.io/badge/Back%20to%20top-â†‘-blue.svg)](#readme)

</div>
